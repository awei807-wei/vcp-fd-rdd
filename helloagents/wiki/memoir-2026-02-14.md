# fd-rdd v0.2.x 压测排障回忆录（2026-02-14）

> 目的：把一次“从误区到可用”的路径讲清楚，便于与他人讨论取舍与下一步。
> 背景：目标是对标 Everything 的体验（常驻可搜、响应快），但在 Linux 上遇到性能与内存的现实约束。

---

## 1. 起点：误区与症状

早期观察到的问题是：

- 只要文件存在变动，就会“缓存/索引”，删除大量数据后内存仍不下降；
- 试图对齐 Everything 的功能后，资源使用反而远超预期，甚至比原生 `fd` 更慢；
- 百万文件压测时出现 4.7GB+ 级别的内存占用，且 RSS 随时间“阶梯上升不回落”。

讨论中逐步确认了关键事实：

- 如果每次查询都全量 Walk 再过滤，那么天然难以接近 `fd` 的效率；
- “删了不降”不一定是泄漏，常见是 allocator/RSS 不回吐 + 容器 capacity 不收缩 + 墓碑/历史结构累积；
- notify 在高频场景不保证事件完全可靠，必须把“丢事件 → 补偿”写进架构。

---

## 2. 路线选择：放弃 WAL，改用 atomic snapshot

调研 WAL crate 后发现成熟度/风险不满足生产要求。

更重要的洞察来自 plocate/fsearch：文件索引不是数据库，索引丢了可以重建，关键是：

- **永远有一个可用版本**（atomic replacement）
- **坏了能识别并回退/重建**

于是采用：

- `index.db.tmp` 写入 → `fsync` → `rename()` 原子替换 → `fsync(dir)`
- header 用 state/version/magic/checksum 判断有效性
- 校验失败则当作无快照，走重建兜底

---

## 3. 可靠性优先：不死锁、不漂移

压测中两个“不可接受”的风险被优先处理：

### 3.1 死锁风险

L2 内部多个锁在读路径与写路径加锁顺序不一致，会在“边写边查”场景互锁卡死。

处理方式：

- 调整查询路径先读取 trigram 候选集（只持有 trigram_index 的锁），再读取 files/tombstones。

### 3.2 事件丢失导致漂移

bounded channel 满时丢事件是合理的背压手段，但必须补偿，否则长跑索引会不一致。

处理方式：

- 记录 overflow_drops；
- overflow 增长触发后台 rebuild（带 cooldown，避免重建风暴）；
- rebuild 清空 L1/L2 后由 L3 全量扫描重建。

---

## 4. 内存优化：用“表示法”而不是“缓存幻想”解决

### 4.1 trigram posting：HashSet → Vec

`HashSet` 的桶/指针开销在百万规模放大非常明显。
改为 `Vec<FileId>` 后常驻内存下降数百 MB 级别（实际以压测为准）。

同时补了一个关键防膨胀点：

- 同 FileId 且同路径的重复 Modify/重复事件只更新元数据，不重复插入 posting，避免 posting 事件风暴下膨胀。

### 4.2 路径反查：去掉 PathBuf 双份存储

为了避免 `files` 与 `path_to_id` 同时持有完整路径字符串，改为：

- `path_hash_to_id`（hash(path) → FileId/少量冲突列表）
- 查找时通过 `files` 的真实路径二次校验保证正确性

并同步升级快照格式：

- 写出 v3（不再落盘 path_to_id）
- 兼容读取 v2（旧快照包含 path_to_id）

### 4.3 快照写入峰值：serialize Vec → serialize_into

快照期间 RSS 阶梯上升的一个来源是序列化时构建巨型 `Vec<u8>`。
改为 `bincode::serialize_into(file)` 流式写入，同时边写边算 checksum/len，并在写完后 seek 回头覆盖 COMMITTED header。

---

## 5. 压测现象的正确解读

- 启动即占用 1~2GB RSS：如果加载了百万文件快照，这是“索引常驻内存”的必然成本；
- 删除大量文件后 RSS 不下降：常见是 allocator 高水位效应 + 容器 capacity 不收缩，并不自动等同泄漏；
- overflow 很大：说明事件风暴已超过 channel 承载，需要依赖 rebuild 兜底保证一致性。

结论：如果目标是“Everything 式常驻快搜”，常驻内存本质上是索引体积 + 表示法；想再降上限，需要进一步走 DocId/压缩 posting/path blob 等路线。

---

## 6. 下一步讨论用的问题清单

1) 目标更像哪种产品？
   - 常驻 Everything（快、占内存）
   - 按需 fd（常驻低、每次扫盘）
   - 守护进程 + worker（常驻低，首次慢随后快）

2) 可接受的资源边界是什么？
   - 每百万文件 RSS 上限？（例如 0.5GB / 1GB / 2GB）
   - 首次查询可接受延迟？（加载快照 vs 从零重建 vs 按需扫描）

3) 是否要为“低占用常驻 + 快搜”投入段式索引？
   - 现有 bincode 快照无法按需加载；
   - 若要常驻极低但仍快搜，需要段式/mmap-friendly 格式（以及 DocId/压缩 posting）。

