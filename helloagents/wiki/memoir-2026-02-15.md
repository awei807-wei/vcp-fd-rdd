# 开发回忆录（2026-02-15）

这份回忆录用于对外交流：把 fd-rdd 从“内存索引原型”推进到“长期 mmap 基座 + LSM”过程中，关键决策、踩坑点与里程碑按时间线记录下来。

## 里程碑回顾

### 阶段 A：把 posting 从“16B 元素”压缩到“4B 元素”

- 关键点：先把 posting 元素统一成 `DocId(u32)`，这一步决定了后续 RoaringBitmap 与紧凑化主表是否成立。
- 结果：posting 从 `Vec<FileKey(dev+ino,16B)>` 变为 `RoaringBitmap(DocId)`；路径从百万次堆分配变为 `PathArena` 连续字节。

### 阶段 A+：动态侧止血（可用性与可观测）

- 事件风暴稳定性：overflow/异常时的 rebuild 不能“连环触发”，必须有 cooldown + 合并策略。
- 原子切换：ArcSwap 让后台构建完成后“一次性换指针”，读请求不中断。
- 观测补齐：Arena/HashMap capacity/Roaring serialized_size 拆项，让优化不再靠猜。

### 阶段 B：v6 段式快照（mmap + lazy decode）

- 目标：冷启动“秒开”。不再把快照反序列化成堆对象，而是 mmap 映射 + OS 按需分页。
- 取舍：posting 采用 Roaring 的 lazy decode（按 trigram 命中 container 才解压），先交付可用锚点。

### 阶段 C：LSM 演进（长期 mmap 基座 + 内存 Delta）

核心转折点：确认“启动不做全量 hydration”，真正把 mmap 段变成长期基座。

- 目录化布局：引入 `index.d/` + `MANIFEST.bin` + 多段文件。
- Flush：把在线变更的内存 Delta 追加为新段，而不是反复写一个大快照。
- delete 语义：v6 tombstone 只对单段 DocId 有效，因此引入 `.del` sidecar（按路径 bytes），解决跨段 delete/rename-from。
- Compaction：段数达到阈值后后台合并成新 base，控制查询合并成本与段数量膨胀。
- 策略：阶段 C 明确“先不补 rkyv”，避免 schema 仍在变化时被 Archived 类型对齐拖慢节奏。

## 当前还值得讨论的遗留点

- L1 缓存键是否要显式切到 DocId：收益与“对外结构泄漏”的边界如何拿捏。
- Manifest/代际（Gen）与严格校验：当段合并/回滚/多进程访问出现时，是否需要更强的 schema 与并发控制。

（更细的完成/未完成清单见 `wiki/stage-progress.md`。）

## 2026-02-15 补记：RSS“死结”拆解（Anonymous vs Private_Clean）

在“新增/删除百万文件”压测中观察到：即使索引条目已大幅减少，进程 RSS 仍可能长时间停留在高水位。进一步用 `/proc/<pid>/smaps_rollup` 拆分后发现，这个现象通常由两部分组成：

1. **Anonymous/Private_Dirty（堆）高水位常驻**  
   - 成因：全量扫描/重建/序列化产生大量临时分配；glibc 往往不会立刻把空闲块归还 OS。
   - 缓解：在 rebuild/full_build 完成瞬间触发一次手动回吐（glibc：`malloc_trim(0)`；mimalloc：`mi_collect(true)`）。

2. **Private_Clean（file-backed）常驻**  
   - 成因：历史 mmap 段（LSM 多段、旧 base）一旦被触页，会形成“干净常驻”下界；重启进程后也可能继续表现为 resident（但可被 OS 丢弃）。
   - 关键结论：把 v6 校验从“先 mmap 再校验”改为“read/seek 流式校验后再 mmap”，只能减少“校验触页”的额外成本；若仍存在大量历史段，Private_Clean 下界依然由段体积决定。
   - 证据：清理旧 `index.db/index.d` 后，`Disk Segments (mmap)` 归零，冷启动 RSS/Private_Clean 立刻显著下降；随后再跑增删压测，曲线变得可解释（新增→上升、删除→回落，但 file-backed 仍取决于段是否被 compaction 清理）。

因此，阶段 C 的下一步真正目标应从“让 mmap 更冷”转向“让历史段更少”：围绕 LSM 的 Flush/Compaction/段清理（GC）建立稳定的运维闭环，使 `Disk Segments` 长期维持在小常数（理想为 1）。

## 2026-02-15 补记：弹性计算接回（AQE for Rebuild）

为降低“全量扫描/重建 = 系统卡顿”的体验成本，本日将 `AdaptiveScheduler` 实际接入 rebuild/full_build：

- rebuild/full_build 启动前先根据系统负载调整目标并行度，并选择执行策略（Serial/Parallel）。
- 扫描层落地为 `ignore` 的 parallel walker（可控 threads），避免在“系统繁忙”时继续扩大并行度带来的抖动。

这为后续“USN/WAL 增量回放 + 弹性校验（VerifyGap）”预留了统一的调度入口。

## 2026-02-15 补记：避免 watcher 反馈回路 + 影子内存可观测

在长期运行时观察到“RSS 随使用时间上涨”的现象，但 L2 估算仍很小。进一步拆分后发现常见根因是：

- watch roots 覆盖了 fd-rdd 自己写入的 snapshot/segment（`index.db/index.d`），导致“索引写入 → watcher 事件 → apply → 再写入”的反馈回路；
- rebuild 期间 pending 事件与跨段 overlay（delete/upsert 屏蔽集合）属于“影子内存”，原内存报告未展示，容易误判为泄漏。

落地改动：

- 事件管道默认忽略 `snapshot_path` 与派生 `index.d/`，并提供 `--ignore-path` 供用户排除日志文件等路径
- MemoryReport 增加 overlay/pending 的统计项（条目数 + 字节数），让 RSS 分解更透明
- rebuild 的 pending_events 按路径去重（仅保留每条路径最新事件），避免 rebuild 期间无限堆积

## 2026-02-15 补记：events.wal（Append-only Log）

为了让系统在 overflow/重启后不必总是走“全盘 rebuild”，补齐了一个追加型事件日志：

- 事件批次在 apply 前写入 `index.d/events.wal`（best-effort；崩溃尾部截断可容忍）
- snapshot flush 时会在边界 `seal()` WAL（切分成 `events.wal.seal-*`），并把 `wal_seal_id` checkpoint 写入 LSM manifest
- 启动加载 segments 后按 checkpoint 回放 WAL，使查询包含“最后一次 snapshot 之后”的变更

这一步把“增量恢复”从纯内存缓冲推进到可持久化账本，为后续 USN/WAL 的更强语义（例如 gap verify、增量校验）奠定落点。

## 2026-02-15 补记：文档同步（README / --help）

为降低使用门槛并避免“文档与实现漂移”，同步更新了：

- README：说明当前持久化布局（`index.d/` + `events.wal`）、AQE 与常见内存观测要点
- CLI `--help`：补齐 `--ignore-path` 与持久化目录布局说明
